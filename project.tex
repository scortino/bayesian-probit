\documentclass{article}
\usepackage[nonatbib, final]{neurips_2020}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\EX}[0]{\mathbb{E}} % expected value
\newcommand{\V}[0]{\mathbb{V}}
\newcommand{\IF}[2]{\mathbbm{1}_{#1}(#2)} % indicator function
\renewcommand{\vec}[1]{\mathbf{#1}}

\title{20592 - Statistics and Probability\\Bayesian Estimation of a Probit Regression Model}
\author{%
  Stefano Cortinovis \\
  \texttt{stefano.cortinovis@studbocconi.it}
  \And
  Daniele Micheletti\\
  \texttt{daniele.micheletti@studbocconi.it}
  \And
  Andrea Teruzzi\\
  \texttt{andrea.teruzzi@studbocconi.it}
  \And
  Leonardo Yang\\
  \texttt{leonardo.yang@studbocconi.it}
}

\begin{document}

\maketitle

\section{Project Introduction}

This report aims at illustrating the use of Bayesian methods for estimating the coefficient of a probit regression model for binary outcomes. 

\section{Probit Model}
Let \(Y_1,...,Y_n\) be a sample of \(n\) independent binary random variables, where \(Y_i \sim B(p_i)\). The probit model assumes that \(p_i = \Phi(\vec{x}_i^T \beta), i = 1,....,n\), where \(\vec{x}_i^T = \begin{pmatrix} x_{i1} & \hdots & x_{1k} \end{pmatrix}\) is a \(k\)-dimensional vector of observed covariates, \(\beta\) is a \(k \times 1\) vector of unknown coefficients, and \(\Phi\) denotes the cumulative distribution function of a standard normal random variable. Given a sample of observations \(\{(y_i, \vec{x}_i)\}_{i=1}^n\), the standard estimation procedure for the probit model employs the maximum likelihood estimation of coefficient \(\beta\). However, the Bayesian approach to the estimation of \(\beta\) constitute an interesting alternative. In particular, suppose the posterior density of \(\beta\), denoted by \(\pi(\beta|\vec{y})\),  was known and tractable. Then, the value of the posterior first moment, \(\EX[\beta|\vec{y}]\), would constitute a reasonable estimate for \(\beta\). However, if we denote the proper or improper prior density of \(\beta\) by \(\pi(\beta)\), the posterior density of \(\beta\) has the form:
\begin{equation*}
    \pi(\beta|\vec{y}) \propto \pi(\beta) \prod_{i=1}^n \Phi(\vec{x}_i^T \beta)^{y_i} (1 - \Phi(\vec{x}_i^T \beta))^{1 - y_i},
\end{equation*}
which is intractable.
\par 
To address the issue of sampling from intractable posteriors, the Metropolis-Hastings algorithm is often employed in Bayesian statistics. By means of a proposal density to used to sample candidates and a probabilistic criterion to accept or reject them, this method is able to generate a Markov chain \(\{\beta\}_{t \geq 1}\) having the target posterior as stationary distribution. This means that, if such a Markov chain of length \(T\) is generated and \(t_0 < T\) denotes the step at which the chain can be considered to have converged to the stationary distribution \(\pi{\beta|\vec{y}}\), the estimator
\begin{equation*}
    \hat{\beta} = \frac{1}{T-t_0} \sum_{t=t_0}^T \beta_t
\end{equation*}
is unbiased and consistent for \(\EX[\beta|\vec{y}]\).
\par
Instances of the MH algorithm differ in terms of the proposal density used. In particular, the methods employed in this report are the Metropolis algorithm and the auxiliary variable Gibbs sampler.

\section{Metropolis Algorithm}
The Metropolis algorithm is an instance of the MH algorithm with proposal density \(q(\beta^*|\beta_t)\) symmetric around the current value of the chain, namely \(\beta_t\). When \(q(y|x)\) has such a property, the candidate acceptance probability becomes:
\begin{equation*}
    \alpha(\beta^*, \beta_t) = \min\Bigg\{1, \frac{\pi(\beta^*|\vec{y})}{\pi(\beta_t|\vec{y})}\Bigg\} = \min\Bigg\{1, \frac{\pi(\beta^*)}{\pi(\beta_t)}\frac{\mathcal{L}(\beta^*)}{\mathcal{L}(\beta_t)}\Bigg\}
\end{equation*}
where \(\mathcal{L}(\beta)\) denotes the likelihood of the model at \(\beta\).
\par
The proposal used in this report is a multivariate normal distribution. In particular, at each sampling step performed by the algorithm, a candidate is drawn from \(q(\beta^*|\beta^t) = N_k(\beta_t,V)\) where \(V = \mathcal{I}^{-1}(\beta_t)\) is the inverse of the Fisher information matrix computed at the current value of \(\beta_t\). Moreover, recall that, in the case of a generalized linear model with link function \(g\), it can be shown that \(\mathcal{I}(\beta_t) = X^T W X\) where \(X = \begin{bmatrix} \vec{x}_1 \hdots \vec{x}_n \end{bmatrix}\) and \(W\) is a diagonal matrix such that 
\begin{equation*}
    w_{ii} = \frac{1}{\V(Y_i)}\bigg(\frac{\partial \vec{x}_i^T \beta}{\partial g(\vec{x}_i^T \beta)}\bigg)^{-2}.
\end{equation*}
This means that, in the case of the probit model, \(w_{ii} = \phi^2(\vec{x}_i^T \beta)/(p_i(1 - p_i))\).
\par 
Algorithm \ref{metropolis} summarizes the steps performed by the Metropolis algorithm presented in this section.

\begin{algorithm}
\caption{Metropolis algorithm for Probit Estimation}\label{metropolis}
\begin{algorithmic}[1]
\Procedure{Metropolis}{$\beta_0$}
    \State $\beta \gets \beta_0$
    \State $m \gets [\beta_0]$
    \Repeat
        \State $\beta^* \gets \beta^* \sim N_k(\beta,\mathcal{I}^{-1}(\beta))$
        \State $u \gets U \sim \text{Unif}(0, 1)$
        \State $\alpha \gets \alpha(\beta, \beta^*) = \min\big\{1, \frac{\pi(\beta^*)}{\pi(\beta)}\frac{\mathcal{L}(\beta^*)}{\mathcal{L}(\beta)}\big\}$
        \If{$u \leq \alpha$}
            \State $\beta \gets \beta^*$
        \EndIf
        \State \textbf{append} $\beta$ to $m$
    \Until {stopping criterion is reached}
    \State \textbf{return} $m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Auxiliary Variable Gibbs Sampler}
Sometimes, when a Gibbs sampler cannot be easily devised for a target distribution, it is possible to exploit a vector of auxiliary random variables to successfully complete the task. This straightforward extension of the simple Gibbs sampler is called auxiliary variable Gibbs sampler. In particular, if the target distribution is \(\pi(\beta|\vec{y})\) and we are able to find a random vector \(\vec{Z}\) such that the full conditionals \(\pi(\beta|\vec{y}, \vec{Z}\) and \(\pi(\vec{Z}|\vec{y}, \beta)\) are known and can be sampled from, then a Gibbs sampler can be used to generate the Markov chain \(\{(\beta_t, \vec{Z}_t)\}_{t\geq 1}\). Crucially, it is possible to show that the sequence \(\{\beta_t\}_{t\geq 1}\) is also a Markov chain and has the target distribution \(\pi(\beta|\vec{y})\) as its stationary distribution. 
\par
In regards to the scenario considered in this report, it turns out that any probit model can be expressed in terms of a normal linear model. In particular, introduce auxiliary vector \(\vec{Z} = \begin{pmatrix} Z_1 \hdots Z_n \end{pmatrix}\) such that the \(Z_i\) are independent \(N(\vec{x}_i^T \beta, 1)\) and define \(Y_i = \IF{(0, \infty)}{Z_i}\), i.e. \(Y_i = 1\) if \(Z_i > 0\) and \(Y_i = 0\) otherwise. Then, it is possible to show that the \(Y_i\) are independent Bernoulli random variables with \(p_i = \Phi(\vec{x}_i^T)\), as prescribed by the probit model. 
\par 
As mentioned above, to use a Gibbs sampler to sample from target \(\pi(\beta|\vec{y})\) using \(\vec{Z}\) as the auxiliary variable, the full conditionals \(\pi(\beta|\vec{y}, \vec{Z})\) and \(\pi(\vec{Z}|\vec{y}, \beta)\) have to be known distributions. First, the full conditional of \(\beta\) is given by:
\begin{equation*}
    \pi(\beta|\vec{y}, \vec{Z}) = \pi(\beta|\vec{Z}) \propto \pi(\beta) \pi(\vec{Z}|\beta) = \pi(\beta) \prod_{i=1}^n\phi(\vec{x}_i^T \beta),
\end{equation*}
which depends on the choice of the prior \(\pi(\beta)\). In particular, if \(\pi(\beta)\) is chosen to be non-informative, standard linear model theory shows that \(\beta|\vec{y}, \vec{Z} \sim N_k(\hat{\beta}_Z, (\vec{X}^T\vec{X})^{-1})\) where \(\hat{\beta}_Z = (\vec{X} \vec{X}^T)^{-1}\vec{X}^T \vec{Z}\). On the other hand, if the conjugate prior \(\beta \sim N_k(\beta^*, \vec{B}^*)\) is chosen, we have that \(\beta | \vec{y}, \vec{Z} \sim N_k(\tilde{\beta}, \tilde{\vec{B}})\) where \(\tilde{\beta} = (\vec{B}^{*-1} + \vec{X}^T\vec{X})^{-1}(\vec{B}^{*-1}\beta^* + \vec{X}^T\vec{Z})\) and \(\tilde{\vec{B}} = (\vec{B}^{*-1} + \vec{X}\vec{X}^T)^{-1}\). 
\par
Next, for each \(i\), the full conditional of \(Z_i\) is given by:
\begin{equation*}
    \pi(Z_i | \vec{y}, \beta) \propto 
    \begin{cases}
        \phi(Z_i; \vec{x}_i^T \beta, 1) \IF{(0, +\infty)}{Z_i} & \text{if } y_i = 1 \\
        \phi(Z_i; \vec{x}_i^T \beta, 1) \IF{(-\infty, 0]}{Z_i} & \text{if } y_i = 0
    \end{cases}
\end{equation*}
that is, \(Z_i\) has a \(N(\vec{x}_i^T \beta, 1)\) truncated by 0 at the left if \(y_i = 0\) and at the right if \(y_i = 1\).
\par 
Notice that, if we choose the prior \(\pi(\beta)\) in one of the two ways mentioned above, both full conditionals have a known distribution and can be easily sampled from. This means that we can indeed exploit \(\vec{Z}\) to use an auxiliary variable Gibbs sampler to sample from \(\pi(\beta|\vec{y})\). Finally, Algorithm \ref{gibbs} summarizes the steps performed by the auxiliary variable Gibbs sampler presented in this section.

\begin{algorithm}
\caption{Auxiliary Variable Gibbs Sampler for Probit Estimation}\label{gibbs}
\begin{algorithmic}[1]
\Procedure{AuxiliaryGibbs}{$\beta_0$}
    \State $\beta \gets \beta_0$
    \State $m \gets [\beta_0]$
    \Repeat
        \State $\vec{Z} \gets \vec{Z} | \vec{y}, \beta \sim \pi(\vec{Z}|\vec{y}, \beta)$
        \State $\beta \gets \beta | \vec{y}, \vec{Z} \sim \pi(\beta | \vec{y}, \vec{Z})$
        \State \textbf{append} $\beta$ to $m$
    \Until {stopping criterion is reached}
    \State \textbf{return} $m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Diagnostics and Performance Comparison}
\section{Final Remarks}

\end{document}
