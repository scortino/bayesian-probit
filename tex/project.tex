\documentclass{article}
\usepackage[final]{neurips_2020}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\EX}[0]{\mathbb{E}} % expected value
\newcommand{\V}[0]{\mathbb{V}}
\newcommand{\IF}[2]{\mathbbm{1}_{#1}(#2)} % indicator function
\renewcommand{\vec}[1]{\mathbf{#1}}

\title{20592 - Statistics and Probability\\Bayesian Estimation of a Probit Regression Model}
\author{%
  Stefano Cortinovis \\
  \texttt{stefano.cortinovis@studbocconi.it}
  \And
  Daniele Micheletti\\
  \texttt{daniele.micheletti@studbocconi.it}
  \And
  Andrea Teruzzi\\
  \texttt{andrea.teruzzi@studbocconi.it}
  \And
  Leonardo Yang\\
  \texttt{leonardo.yang@studbocconi.it}
}

\begin{document}

\maketitle

\section{Project Introduction}

This report aims at illustrating the use of Bayesian methods for estimating the coefficient of a probit regression model for binary outcomes. Section 2 succinctly describes the probit regression model and the Bayesian approach to estimating its coefficients using instances of the Metropolis-Hastings algorithm. Next, section 3 and 4 briefly explain two such instances, namely the Metropolis algorithm and the auxiliary variable Gibbs sampler. Then, section 5 compares the performance of the two algorithms on a dataset provided by Finney \cite{finney1947estimation} to study the relationship between the occurrence of transient skin vasorestriction and the rate and volume of the air inspired by the individuals undergoing the test. Performance is assessed by means of basic diagnostics, such as trace plots and acceptance rate computation. Lastly, section 6 summarizes the results of the project. As prescribed by the project's description, this report is heavily inspired by Albert and Chib \cite{albert1993bayesian}.

\section{Probit Model}
Let \(Y_1,...,Y_n\) be a sample of \(n\) independent binary random variables, where \(Y_i \sim B(p_i)\). The probit model assumes that \(p_i = \Phi(\vec{x}_i^T \beta), i = 1,....,n\), where \(\vec{x}_i^T = \begin{pmatrix} x_{i1} & \hdots & x_{1k} \end{pmatrix}\) is a \(k\)-dimensional vector of observed covariates, \(\beta\) is a \(k \times 1\) vector of unknown coefficients, and \(\Phi\) denotes the cumulative distribution function of a standard normal random variable. Given a sample of observations \(\{(y_i, \vec{x}_i)\}_{i=1}^n\), the standard estimation procedure for the probit model employs the maximum likelihood estimation of coefficient \(\beta\). However, the Bayesian approach to the estimation of \(\beta\) constitutes an interesting alternative. In particular, suppose the posterior density of \(\beta\), denoted by \(\pi(\beta|\vec{y})\),  was known and tractable. Then, the value of the posterior first moment, \(\EX[\beta|\vec{y}]\), would constitute a reasonable estimate for \(\beta\). However, if we denote the proper or improper prior density of \(\beta\) by \(\pi(\beta)\), the posterior density of \(\beta\) has the form:
\begin{equation*}
    \pi(\beta|\vec{y}) \propto \pi(\beta) \prod_{i=1}^n \Phi(\vec{x}_i^T \beta)^{y_i} (1 - \Phi(\vec{x}_i^T \beta))^{1 - y_i},
\end{equation*}
which is intractable.
\par 
To address the issue of sampling from intractable posteriors, the Metropolis-Hastings algorithm is often employed in Bayesian statistics. By means of a proposal density used to sample candidates and a probabilistic criterion to accept or reject them, this method is able to generate a Markov chain \(\{\beta_t\}_{t \geq 1}\) having the target posterior as stationary distribution. This means that, if such a Markov chain of length \(T\) is generated and \(t_0 < T\) denotes the step at which the chain can be considered to have converged to the stationary distribution \(\pi({\beta|\vec{y}})\), the estimator
\begin{equation*}
    \hat{\beta} = \frac{1}{T-t_0} \sum_{t=t_0}^T \beta_t
\end{equation*}
is unbiased and consistent for \(\EX[\beta|\vec{y}]\).
\par
Instances of the MH algorithm differ in terms of the proposal density used. In particular, the methods employed in this report are the Metropolis algorithm and the auxiliary variable Gibbs sampler.

\section{Metropolis Algorithm}
The Metropolis algorithm is an instance of the MH algorithm with proposal density \(q(\beta^*|\beta_t)\) symmetric around the current value of the chain, namely \(\beta_t\). When \(q(y|x)\) has such a property, the candidate acceptance probability becomes:
\begin{equation*}
    \alpha(\beta_t, \beta^*) = \min\Bigg\{1, \frac{\pi(\beta^*|\vec{y})}{\pi(\beta_t|\vec{y})}\Bigg\} = \min\Bigg\{1, \frac{\pi(\beta^*)}{\pi(\beta_t)}\frac{\mathcal{L}(\beta^*)}{\mathcal{L}(\beta_t)}\Bigg\}
\end{equation*}
where \(\mathcal{L}(\beta)\) denotes the likelihood of the model at \(\beta\).
\par
The proposal used in this report is a multivariate normal distribution. In particular, at each sampling step performed by the algorithm, a candidate is drawn from \(q(\beta^*|\beta_t) = N_k(\beta_t,V)\) where \(V = \mathcal{I}^{-1}(\beta_t)\) is the inverse of the Fisher information matrix computed at the current value of \(\beta_t\). Moreover, recall that, in the case of a generalized linear model with link function \(g\), it can be shown that \(\mathcal{I}(\beta_t) = X^T W X\) where \(X = \begin{bmatrix} \vec{x}_1 \hdots \vec{x}_n \end{bmatrix}\) and \(W\) is a diagonal matrix such that 
\begin{equation*}
    w_{ii} = \frac{1}{\V(Y_i)}\bigg(\frac{\partial \vec{x}_i^T \beta}{\partial g(\vec{x}_i^T \beta)}\bigg)^{-2}.
\end{equation*}
This means that, in the case of the probit model, \(w_{ii} = \phi^2(\vec{x}_i^T \beta)/(p_i(1 - p_i))\).
\par 
Algorithm \ref{metropolis} summarizes the steps performed by the Metropolis algorithm presented in this section.

\begin{algorithm}
\caption{Metropolis Algorithm for Probit Estimation}\label{metropolis}
\begin{algorithmic}[1]
\Procedure{Metropolis}{$\beta_0$}
    \State $\beta \gets \beta_0$
    \State $m \gets [\beta_0]$
    \Repeat
        \State $\beta^* \gets \beta^* \sim N_k(\beta,\mathcal{I}^{-1}(\beta))$
        \State $u \gets U \sim \text{Unif}(0, 1)$
        \State $\alpha \gets \alpha(\beta, \beta^*) = \min\big\{1, \frac{\pi(\beta^*)}{\pi(\beta)}\frac{\mathcal{L}(\beta^*)}{\mathcal{L}(\beta)}\big\}$
        \If{$u \leq \alpha$}
            \State $\beta \gets \beta^*$
        \EndIf
        \State \textbf{append} $\beta$ to $m$
    \Until {stopping criterion is reached}
    \State \textbf{return} $m$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Auxiliary Variable Gibbs Sampler}
Sometimes, when a Gibbs sampler cannot be easily devised for a target distribution, it is possible to exploit a vector of auxiliary random variables to successfully complete the task. This straightforward extension of the simple Gibbs sampler is called auxiliary variable Gibbs sampler. In particular, if the target distribution is \(\pi(\beta|\vec{y})\) and we are able to find a random vector \(\vec{Z}\) such that the full conditionals \(\pi(\beta|\vec{y}, \vec{Z})\) and \(\pi(\vec{Z}|\vec{y}, \beta)\) are known and can be sampled from, then a Gibbs sampler can be used to generate the Markov chain \(\{(\beta_t, \vec{Z}_t)\}_{t\geq 1}\). Crucially, it is possible to show that the sequence \(\{\beta_t\}_{t\geq 1}\) is also a Markov chain and has the target distribution \(\pi(\beta|\vec{y})\) as its stationary distribution. 
\par
In regards to the scenario considered in this report, it turns out that any probit model can be expressed in terms of a normal linear model. In particular, introduce auxiliary vector \(\vec{Z} = \begin{pmatrix} Z_1 \hdots Z_n \end{pmatrix}\) such that the \(Z_i\) are independent \(N(\vec{x}_i^T \beta, 1)\) and define \(Y_i = \IF{(0, \infty)}{Z_i}\), i.e. \(Y_i = 1\) if \(Z_i > 0\) and \(Y_i = 0\) otherwise. Then, it is possible to show that the \(Y_i\) are independent Bernoulli random variables with \(p_i = \Phi(\vec{x}_i^T\beta)\), as prescribed by the probit model. 
\par 
As mentioned above, to use a Gibbs sampler to sample from target \(\pi(\beta|\vec{y})\) using \(\vec{Z}\) as the auxiliary variable, the full conditionals \(\pi(\beta|\vec{y}, \vec{Z})\) and \(\pi(\vec{Z}|\vec{y}, \beta)\) have to be known distributions. First, the full conditional of \(\beta\) is given by:
\begin{equation*}
    \pi(\beta|\vec{y}, \vec{Z}) = \pi(\beta|\vec{Z}) \propto \pi(\beta) \pi(\vec{Z}|\beta) = \pi(\beta) \prod_{i=1}^n\phi(Z_i;\vec{x}_i^T \beta, 1),
\end{equation*}
which depends on the choice of the prior \(\pi(\beta)\). In particular, if \(\pi(\beta)\) is chosen to be non-informative, we have that
\begin{align*}
    \pi(\beta|\vec{y}, \vec{Z}) &\propto \prod_{i=1}^n\phi(Z_i;\vec{x}_i^T \beta, 1) \propto \exp\Bigg\{\sum_{i=1}^n (Z_i - \vec{x}_i^T \beta)^2\Bigg\} \\
                                &\propto \exp\Big\{(\vec{X} \beta)^T (\vec{X} \beta) - 2 (\vec{X}\beta)^T \vec{Z}\Big\} \\
                                &= \exp\Big\{(\vec{X} \beta)^T (\vec{X} \beta) - 2 (\vec{X}\beta)^T 
                                \vec{X} (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\Big\}.
\end{align*}
By completing the square with the product of the transpose of \(\vec{X} (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\) with itself,
\begin{align*}
    \pi(\beta|\vec{y}, \vec{Z}) &\propto \exp\Big\{\big(\vec{X}\beta - \vec{X} (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\big)^T\big(\vec{X}\beta - \vec{X} (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\big)\Big\} \\
    &= \exp\Big\{\big(\beta - (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\big)^T (X^T X) \big(\beta - (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\big)\Big\}
\end{align*}
which is the kernel of a multivariate normal random variable with with mean \(\hat{\beta}_Z = (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{Z}\) and variance \((\vec{X}^T \vec{X})^{-1}\). That is, if \(\pi(\beta)\) is chosen to be non-informative, \(\beta|\vec{y}, \vec{Z} \sim N_k(\hat{\beta}_Z, (\vec{X}^T\vec{X})^{-1})\). On the other hand, if the conjugate prior \(\beta \sim N_k(\beta^*, \vec{B}^*)\) is chosen, we it can be shown that \(\beta | \vec{y}, \vec{Z} \sim N_k(\tilde{\beta}, \tilde{\vec{B}})\) where \(\tilde{\beta} = (\vec{B}^{*-1} + \vec{X}^T\vec{X})^{-1}(\vec{B}^{*-1}\beta^* + \vec{X}^T\vec{Z})\) and \(\tilde{\vec{B}} = (\vec{B}^{*-1} + \vec{X}^T\vec{X})^{-1}\). 
\par
Next, for each \(i\), the full conditional of \(Z_i\) is given by:
\begin{equation*}
    \pi(Z_i | \vec{y}, \beta) \propto 
    \begin{cases}
        \phi(Z_i; \vec{x}_i^T \beta, 1) \IF{(0, +\infty)}{Z_i} & \text{if } y_i = 1 \\
        \phi(Z_i; \vec{x}_i^T \beta, 1) \IF{(-\infty, 0]}{Z_i} & \text{if } y_i = 0
    \end{cases}
\end{equation*}
that is, \(Z_i\) has a \(N(\vec{x}_i^T \beta, 1)\) truncated by 0 at the left if \(y_i = 1\) and at the right if \(y_i = 0\).
\par 
Notice that, if we choose the prior \(\pi(\beta)\) in one of the two ways mentioned above, both full conditionals have a known distribution and can be easily sampled from. This means that we can indeed exploit \(\vec{Z}\) to use an auxiliary variable Gibbs sampler to sample from \(\pi(\beta|\vec{y})\). Finally, Algorithm \ref{gibbs} summarizes the steps performed by the auxiliary variable Gibbs sampler presented in this section.

\begin{algorithm}
\caption{Auxiliary Variable Gibbs Sampler for Probit Estimation}\label{gibbs}
\begin{algorithmic}[1]
\Procedure{AuxiliaryGibbs}{$\beta_0$}
    \State $\beta \gets \beta_0$
    \State $m \gets [\beta_0]$
    \Repeat
        \State $\vec{Z} \gets \vec{Z} | \vec{y}, \beta \sim \pi(\vec{Z}|\vec{y}, \beta)$
        \State $\beta \gets \beta | \vec{y}, \vec{Z} \sim \pi(\beta | \vec{y}, \vec{Z})$
        \State \textbf{append} $\beta$ to $m$
    \Until {stopping criterion is reached}
    \State \textbf{return} $m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Diagnostics and Performance Comparison}
% summary of procedures (test MH and Gibbs on same dataset(Finney47) and use of 2 priors for Gibbs(noninfo and multinorm) and 3 for MH(noninfo, lowvar multinorm, highvar multinorm)).
% % distplot comparison with paper,

Following the pseudo-code described in Algorithm \ref{metropolis} and \ref{gibbs} we implemented the Metropolis and auxiliary variable Gibbs sampler algorithms using the Python programming language. In both cases, the user is allowed to specify whether to include an intercept in the estimation, as well as the initial value for \(\beta_0\), with the default being the OLS estimate for \(\beta\). Moreover, for the Metropolis algorithm, the user is allowed to specify a custom prior distribution, while the Gibbs sampler implemented the full condition for the non-informative and conjugate multivariate normal prior. 
% We tested them on the same dataset Finney47 \cite{finney1947estimation}. 

In particular, we fitted a BayesianProbit model through the Metropolis-Hasting algorithm  on the Finney47 dataset \cite{finney1947estimation} in 3 different scenarios:
\begin{enumerate}
    \item Non-informative prior 
    \item Multivariate normal prior with high variance centered around the true values of the betas
    \item Multivariate normal prior with low variance centered around the true values of the betas
\end{enumerate}

On the same dataset, we also tested the Auxiliary Variable Gibbs Sampler in 2 different scenarios:
\begin{enumerate}
    \item Non-informative prior
    \item Multivariate normal prior with high variance (diagonal covariance matrix with diagonal entries equal to 10) centered around 0
    \item Multivariate normal prior with low variance (diagonal covariance matrix with diagonal entries equal to 1) centered around 0
\end{enumerate}

Both the algorithms were run assuming the existence of an intercept in the model, with \(\beta_0 = \hat\beta_{OLS} = (\vec{X}^T \vec{X})^{-1}\vec{X}^T \vec{y}\), for a number of iterations equal to XXXX. Moreover, we decided to create a variable named "warmup" that would store the number of initial observations to be discarded to compute the previously described \(\hat\beta\).

As a result of these simulations, we plotted the distribution and the trace of the parameters obtained in each scenario. We also plotted the behaviour of the acceptance rate for the Metropolis algorithm.  

% distplots are different in shape but similar scale, is it good?

% Describe traceplots (Gibbs wiggle wiggle),
% acceptance rate (0.4, not too high(local), not too low(stuck) -> slow convergence to stationary distribution, tao choice)
First, we compared the trace plots obtained in the first scenario of the two algorithms. We could immediately notice that the Gibbs Sampler trace plot seemed to be more accurate as a representation of a sample drawn from the posterior of our parameter of interest. The main reason is that, since the Gibbs Sampler always accepts the proposal, it seems to better explore the sample space of \(\beta\). The Metropolis algorithm, instead, accepts the proposal based on an acceptance ratio and this means that sometimes the proposal is rejected and the new state of the Markov Chain is the same as in the previous iteration. From the trace plot we can see that this stochastic process explores the sample space of \(\beta\) based on "local movements" which make the next observation more likely to be correlated with the previous one. 

(Therefore, if the aim was to obtain a sample of independent observations we would recommend to perform a systematic random sampling on the output sample.) %makes sense ?)  

Moreover, we also noticed that within the simulations performed using Metropolis, as expected, the prior with high variance outperformed the one with low variance. Regarding the Gibbs Sampler, the scenario with multivariate normal prior produced a better trace plot then the one with non informative prior.     
% Firstly, we compared the trace plots. We denote that the Metropolis chain, compared to the Gibbs Sampler one, gets more frequently stuck in certain neighbourhood of the distribution.
% The Gibbs Sampler has a more wiggly shape that guarantees a less local exploration of the distribution of the parameters. 

%add something about the speed to the convergence? Using warmup we skipped this topic

Looking at the acceptance ratio of the Metropolis it is possible to denote a stabilization of the rate around 0.4 after XXXX iterations. This value can be considered quite satisfactory. On one hand it is not too high, which would lead to the risk of a local exploration. On the other hand it is not too low, that would lead to the risk of a slow convergence to the stationary distribution.

%tao choice?

\section{Final Remarks}

\bibliographystyle{plain}
\bibliography{project}

\end{document}
